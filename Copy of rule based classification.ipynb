{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"T-oMji5d1ttb"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t00qTpMV1ttc"},"outputs":[],"source":["pd.options.mode.chained_assignment = None  # default='warn'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JxJspKg91ttc"},"outputs":[],"source":["df=pd.read_csv('Reviews.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jragm0bu1ttc"},"outputs":[],"source":["#preparing dataset and cleaing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E0uQB7Ph1ttc"},"outputs":[],"source":["df_new=df[['Summary','Score']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UqobCACb1ttd","outputId":"99687699-624c-479f-8bab-00988e47b48d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Summary</th>\n","      <th>Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Good Quality Dog Food</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Not as Advertised</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"Delight\" says it all</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Cough Medicine</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Great taffy</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 Summary  Score\n","0  Good Quality Dog Food      5\n","1      Not as Advertised      1\n","2  \"Delight\" says it all      4\n","3         Cough Medicine      2\n","4            Great taffy      5"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df_new.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwtkRWyG1ttd"},"outputs":[],"source":["df_new['Score'].replace({1: 'negative', 2: 'negative', 3: 'neutral', 4: 'positive', 5: 'positive'}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3XR3cDU1ttd","outputId":"148f780d-be66-406e-eb2c-4c83d304a411"},"outputs":[{"data":{"text/plain":["positive    443777\n","negative     82037\n","neutral      42640\n","Name: Score, dtype: int64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df_new['Score'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vx_kQIOO1ttd","outputId":"2616b3f3-1833-44be-a99b-50aa80d41e53"},"outputs":[{"data":{"text/plain":["Summary    27\n","Score       0\n","dtype: int64"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df_new.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kZhGvmp1ttd"},"outputs":[],"source":["df_new.dropna(subset=['Summary'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uj3ud1pO1ttd","outputId":"3a7fa647-02c9-4776-8d2f-e245deeae32d"},"outputs":[{"data":{"text/plain":["Summary    0\n","Score      0\n","dtype: int64"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["df_new.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5Gb-lF91tte"},"outputs":[],"source":["df_new['Summary'] = df_new['Summary'].str.lower()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YP0GCUg_1tte"},"outputs":[],"source":["#spliting data into train,validation,test datasets\n","X_tr=df_new.loc[:0.8*(len(df_new)),'Summary']\n","Y_tr=df_new.loc[:0.8*(len(df_new)),'Score']\n","X_dev=df_new.loc[0.8*(len(df_new)):0.9*(len(df_new)),'Summary']\n","Y_dev=df_new.loc[0.8*(len(df_new)):0.9*(len(df_new)),'Score']\n","X_te=df_new.loc[0.9*(len(df_new)):,'Summary']\n","Y_te=df_new.loc[0.9*(len(df_new)):,'Score']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1jaPJpY1tte","outputId":"3b2eeb92-ddfd-40ba-9ef1-e7f94ff23e74"},"outputs":[{"data":{"text/plain":["((454719,), (454719,), (56840,), (56840,), (56868,), (56868,))"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["X_tr.shape,Y_tr.shape,  X_dev.shape,Y_dev.shape,   X_te.shape,Y_te.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQrCC8aU1tte"},"outputs":[],"source":["#rule based classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6W8NSDSL1tte"},"outputs":[],"source":["#specifying rules\n","positive_words = ['large','delicious', 'amazing', 'excellent', 'fantastic', 'wonderful', 'tasty', 'awesome', 'great', 'love', 'perfect',\n","                  'enjoyable', 'yum', 'outstanding', 'superb', 'best', 'heavenly', 'satisfying', 'mouthwatering', 'terrific', 'top-notch','yummy','good','nice']\n","negative_words = ['small','not','disappointing', 'bad', 'poor', 'awful', 'horrible', 'mediocre', 'disgusting', 'terrible', 'unpleasant', 'bland',\n","                  'overrated', 'subpar', 'unappetizing', 'disliked', 'unimpressive', 'displeasing', 'underwhelming', 'rancid', 'worse', 'regrettable']\n","neutral_words = ['average', 'typical', 'ordinary', 'standard', 'common', 'usual', 'okay', 'acceptable', 'adequate', 'moderate',\n","                 'indifferent', 'so-so', 'plain', 'routine', 'inoffensive', 'fair', 'balanced', 'unspectacular', 'neutral', 'decent']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znOvUv6E1tte"},"outputs":[],"source":["def rule_model(X,Y):\n","    #assigning scores\n","    k=0\n","    count=[0] * len(X)\n","    for i in X:\n","        words=i.split()\n","        for word in words:\n","            if word in positive_words:\n","                count[k]+= 10\n","            if word in negative_words:\n","                count[k]-=10\n","            if word in neutral_words:\n","                count[k]+=5\n","        k+=1\n","\n","    #decision making\n","    for i in range(len(count)):\n","        if count[i]>=10:\n","            count[i]='positive'\n","        elif count[i] >=0 and count[i] <=10:\n","            count[i]='neutral'\n","        else:\n","            count[i]='negative'\n","\n","    #convert data frame to list\n","    ground_truth = Y.tolist()\n","\n","    #accuracy for data\n","    pred=0\n","    for i in range(len(ground_truth)):\n","        if ground_truth[i]==count[i]:\n","            pred+=1\n","    accuracy=pred/len(X)\n","\n","    return accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EOPyfPho1tte","outputId":"a5fbb1b5-8a07-4ff5-cefd-883b45567627"},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy for train data is : 0.44286251509173796 \n","accuracy for validation data is : 0.44799437016185784 \n","accuracy for test data is : 0.44445030597172397 \n"]}],"source":["print('accuracy for train data is : {0} ' .format(rule_model(X_tr,Y_tr)))\n","print('accuracy for validation data is : {0} ' .format(rule_model(X_dev,Y_dev)))\n","print('accuracy for test data is : {0} ' .format(rule_model(X_te,Y_te)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddbA0bvs1tte"},"outputs":[],"source":["#even if we randomly assign each review to positive,neural,negative we can get 33% accuracy ,here accuracy is around 44%\n","# HOW TO INCREASE ACCURACY?\n","#make positive,negatuve,neutral lists effective and one can play with scores assignment."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"provenance":[{"file_id":"https://github.com/prabhas2002/text-classification/blob/main/rule%20based%20classification.ipynb","timestamp":1711172678681}]}},"nbformat":4,"nbformat_minor":0}